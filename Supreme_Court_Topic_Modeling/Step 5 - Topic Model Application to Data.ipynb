{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NMF Application to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Workflow\n",
    "\n",
    "Step 1: run the model against the entire dataframe to collect the topics\n",
    "\n",
    "Step 2: take this model and apply it back to the dataframe to assign most likely topic to each case (we want the topic # and its dot product)\n",
    "\n",
    "Step 3: make a dictionary of the components that make up each topic from the original model\n",
    "\n",
    "Step 4: use this dictionary to \"look up\" the topic components and apply those to the dataframe\n",
    "\n",
    "Step 5: Getting data together for visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##########################################  modeling imports  #######################################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "#from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"full_proj_lemmatized3.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/full-project.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.ix[15000, \"case_url\"]\n",
    "#'http://caselaw.findlaw.com/us-supreme-court/382/12.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Run model against entire dataframe (as a corpus)Â¶\n",
    "Think of it like this: We need to find the themes across the entire set of documents (over 23,000 in all), so how else would we do this than stacking every document together as a reservoir to extract information out of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def nmf_mod(corp ):\n",
    "    df = .80\n",
    "    n_topics = 30\n",
    "    n_features = 2000\n",
    "    n_top_words = 40\n",
    "    \n",
    "    # Use tf-idf features for NMF.\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=df, min_df=5, # ngram_range=(1,2), #max_features=n_features,\n",
    "                                       stop_words='english')\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corp)\n",
    "\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model with tf-idf features, \"\n",
    "          \"n_topics= %d, n_topic_words= %d, n_features= %d...\"\n",
    "          % (n_topics, n_top_words, n_features))\n",
    "\n",
    "    nmf = NMF(n_components=n_topics, random_state=2, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    \n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    #return print_top_words(nmf, tfidf_feature_names, n_top_words) \n",
    "    return tfidf,nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf, nmf_mod_test = nmf_mod(df.lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Applying the model back to the dataframe\n",
    "\n",
    "NMF (as well as other types of topic modeling) returns a matrix of likelihoods that a particular document fits in Topic 1, 2, etc. Unlike LDA, An NMF matrix does not contain probabilities of inclusion, but rather the dot product of two matrices. Don't worry about the (linear algebra) details, just imagine that we need to find the biggest number in this matrix and return the index of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "out =nmf_mod_test.transform(tfidf)\n",
    "out[49] #verified that each of these is different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/dataframe-array.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Returning these as a Series** \n",
    "It's easy to run the model against a column of the dataframe, return it as a series, and append that series as a new column. (remember not to sort if you do this because you need the order to stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "topics = []\n",
    "for item in out:\n",
    "    max_index, max_value = max(enumerate(item), key=operator.itemgetter(1))\n",
    "    topics.append(max_index) \n",
    "    \n",
    "df[\"topicnumber\"] = pd.Series(topics, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "topics_likelihood = []\n",
    "for item in out:\n",
    "    max_index, max_value = max(enumerate(item), key=operator.itemgetter(1))\n",
    "    topics_likelihood.append(max_value)\n",
    "    \n",
    "df[\"strengthoftopic\"] = pd.Series(topics_likelihood, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.topicnumber.value_counts() #let's make sure this is a good model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/topic-count-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Creating dictionary of topic components\n",
    "\n",
    "There's probably an easier way to do this, but I haven't found one. I'm running the model function again (random state will get the same results as before) but this time creating a topic words feature space to \"look up\" in my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def nmf_topics_dict(corp, n_topics):\n",
    "    df = .80\n",
    "    n_top_words = 40\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=df, min_df=5,# ngram_range=(1,2), #max_features=n_features,\n",
    "                                       stop_words='english')\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corp)\n",
    "    nmf = NMF(n_components=n_topics, random_state=2, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "      \n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        topic_dict[topic_idx] = \", \".join([tfidf_feature_names[i] \\\n",
    "                                    for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# After testing different topic distributions, 30 was optimal\n",
    "nmf_words_30 = nmf_topics_dict(df.lem, 30) #dict object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nmf_words_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/topic-store.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('finaliteration_topics.json', 'w') as fp:\n",
    "    json.dump(nmf_words_30, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Looking up topic words for each item in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def word_lookup(num):\n",
    "    return nmf_words_30.get(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[\"words\"] = df.topicnumber.apply(word_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.ix[15017,\"words\"] # This cell and the one below verifies that it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/word-lookup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.ix[14972,\"lem\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../images/lem-lookup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.ix[15017,\"case_url\"]\n",
    "# 'http://caselaw.findlaw.com/us-supreme-court/380/145.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"full_project_modelled_final.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"full_project_modelled_final.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Arranging data for visualization\n",
    "\n",
    "Creating a brushable area chart with D3 requires a datapoint for every topic for every year, so we have to do some pivoting to make that happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# some topics were extremely similar and at the suggestion of my instructors,\n",
    "# for the sake of the visualization, I have condensed the topics to 20\n",
    "\n",
    "def topic_condenser(topicnum):\n",
    "    if topicnum == 20:\n",
    "        return 24\n",
    "    if topicnum == 25:\n",
    "        return 1\n",
    "    if topicnum == 2:\n",
    "        return 12\n",
    "    if topicnum == 27:\n",
    "        return 26\n",
    "    if topicnum == 18 or topicnum == 5:\n",
    "        return 29\n",
    "    if topicnum == 8 or topicnum == 22:\n",
    "        return 7\n",
    "    if topicnum == 15:\n",
    "        return 16\n",
    "    if topicnum == 9:\n",
    "        return 14\n",
    "    if topicnum == 19:\n",
    "        return 3\n",
    "    else: \n",
    "        return topicnum\n",
    "df[\"condensedtopics\"] = df.topicnumber.apply(topic_condenser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# doing some research on the not so obvious topics\n",
    "df = df[df[\"topicnumber\"] != 2]\n",
    "#df_16.ix[15065, \"caseurl\"]\n",
    "df_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/research.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_details = pd.read_csv(\"detailsford3.csv\", encoding = 'iso-8859-1')\n",
    "df_details.columns = [\"condensedtopics\", \"topicname\", \"title\", \"exampleURL\", \"leadpp\", \"topicwords\"]\n",
    "df_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/research2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_with_details = pd.merge(df, df_details, how = \"inner\", on = \"condensedtopics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#temp_df = df_with_details[['years', 'condensedtopics', \"topicname\", \"title\", \"exampleURL\", \"leadpp\", \"topicwords\"]]\n",
    "#temp_df.to_csv(\"temp.csv\")\n",
    "temp_df = df_with_details[['years', 'condensedtopics']]\n",
    "temp_df.condensedtopics.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/condensed-count.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#dummy value for each existing topic. Pay no attention to this error.\n",
    "temp_df[\"count\"] = 1\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#this condenses each point for the same year into n number of points \n",
    "temp_df = temp_df.groupby([\"years\", \"condensedtopics\"]).count().reset_index()\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/temp-df.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_fillna = temp_df.pivot_table(\"count\", \"years\", \"condensedtopics\").fillna(0).unstack().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A few (really cool) things are happening**\n",
    "\n",
    "First, we are pivoting to add dummy values for nonexistent year/topic points (for ex, there's only 1 case in 1792 but 30 topics, we need 29 points of 0). The topic numbers become column headers first, followed by filling the NaNs with 0's, then we stack the df back to the way it was and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#we lose the count label column in the previous steps, so we're just renaming it here, and reordering columns based on \n",
    "#how they are arranged in the viz csv\n",
    "data_fillna.columns = [\"condensedtopics\", \"years\", \"count\"]\n",
    "data_fillna = data_fillna[[\"years\", \"condensedtopics\", \"count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#merge data\n",
    "final_data = pd.merge(data_fillna, df_details, how = \"inner\", on = \"condensedtopics\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/merge-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#sort by year\n",
    "final_data.sort_values(\"years\", inplace = True, ascending = True)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/sort-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#backup file\n",
    "final_data.to_csv(\"topicsbyyear.csv\", index = False)\n",
    "final_data.to_csv(\"year_topic_data2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''the best part of this viz is the brushing side to side effect. For that, we need total cases for every year\n",
    "and need no other information'''\n",
    "\n",
    "data_fillna.groupby(\"years\")[\"count\"].sum().reset_index().to_csv(\"year_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/visualization.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
