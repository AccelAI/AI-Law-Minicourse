{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Software Package & Built in Function Documentation\n",
    "- Scikit Learn (sklearn) - http://scikit-learn.org/stable/documentation.html\n",
    "- Natural Language Toolkit (NLTK) - http://www.nltk.org/\n",
    "- SpaCy - https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Preprocessing Workflow\n",
    "\n",
    "- Removing state names\n",
    "- Removing case names\n",
    "- Removing common stopwords (for example, \"the\" isn't a useful word)\n",
    "- Removing people's names (loading the baby name dataset from sklearn)\n",
    "- Removing day of the week, month names - this throws off our model into thinking we care about period of time\n",
    "- Stripping non-words (lots of numbers referencing other cases - another interesting project could be keeping ONLY nums)\n",
    "- Lemmatizing (getting the root of a word - ie run out of running)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a note about stopwords if you've never done NLP before - there's no one size fits all stopwords subsitution list for knowledge of your domain and which words should be excluded (or kept). Creating a powerful stopwords list is an interative process and requires a lot of stopchecking of your actual data to do it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# If you get import module errors regarding spacy or en - run these commands\n",
    "# %%bash\n",
    "# pip install spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy validate\n",
    "# python -m spacy download en\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.corpus import names as names\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assign Variables to Common Words to Creat Stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "male_names = [w.lower() for w in male_names]\n",
    "male_names_plur = [(w.lower() + \"s\") for w in male_names]\n",
    "female_names_plur = [(w.lower() + \"s\") for w in female_names]\n",
    "female_names = [w.lower() for w in female_names]\n",
    "casenames = list(pd.read_csv(\"casetitles.csv\",encoding = 'iso-8859-1'))\n",
    "statenames = list(pd.read_csv(\"statenames.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "homespun_words = ['join', 'seek', 'ginnane', 'kestenbaum', 'hummel', 'loevinger', 'note', 'curiam', 'mosk', 'pd', \\\n",
    "                'paxton', 'rhino', 'buchsbaum', 'hirshowitz', 'misc', 'assistant', 'whereon', 'dismiss', 'sod', \\\n",
    "                'vote', 'present', 'entire', 'frankfurter', 'ante', 'leave', 'concur', 'entire', 'mootness', \\\n",
    "                'track', 'constitution', 'jj', 'blackmun', 'rehnquist', 'amici,sup', 'rep', 'stat', 'messes', \\\n",
    "                'like', 'rev', 'trans', 'bra', 'teller', 'vii', 'erisa', 'usca', 'annas', 'lead', 'cf', 'cca', \\\n",
    "                'fsupp', 'afdc', 'amicus', 'ante', 'orrick', 'kansa', 'pd', 'foth', 'stucky', 'aver',\"united\", \\\n",
    "                \"may\", \"argued\", \"argue\", \"decide\", \"rptr\", \"nervine\", \"pp\",\"fd\" ,\"june\", \"july\", \\\n",
    "                \"august\", \"september\", \"october\", \"november\", \"states\", \"ca\", \"joyce\", \"certiorari\", \"december\",\\\n",
    "                \"january\", \"february\", \"march\", \"april\", \"writ\", \"supreme court\", \"court\", \"dissent\", \\\n",
    "                \"opinion\", \"footnote\",\"brief\", \"decision\", \"member\", \"curiam\", \"dismiss\", \"note\", \"affirm\", \\\n",
    "                \"question\", \"usc\", \"file\"]\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + list(homespun_words) + list(ENGLISH_STOP_WORDS) \\\n",
    "               + list(statenames) + list(casenames) + list(female_names) + list(male_names) + \\ \n",
    "               list(female_names_plur) + list(male_names_plur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Cleaner (including stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "STOPLIST = set(list(stopwords.words('english')) + list(sub_list) + list(ENGLISH_STOP_WORDS))\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    separators = [\"\\xa0\\xa0\\xa0\\xa0\", \"\\r\", \"\\n\", \"\\t\", \"n't\", \"'m\", \"'ll\", '[^a-z ]']\n",
    "    for i in separators:\n",
    "        sample = re.sub(i, \" \", sample.lower())\n",
    "        \n",
    "    ## get the tokens using spaCy - this makes it possible to lemmatize the words\n",
    "    tokens = nlp(sample)\n",
    "    tokens = [tok.lemma_.strip() for tok in tokens]\n",
    "\n",
    "    ## apply our stoplist\n",
    "    return [tok for tok in tokens if len(tok) != 1 and tok not in STOPLIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc_list[\"lem\"] = doc_list.case.apply(text_processing)\n",
    "doc_list.to_pickle(\"full_proj_lemmatized.pickle\") ## to be used in model selection"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
